{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f86582a6",
   "metadata": {},
   "source": [
    "# 💥 Berefore you run this notebook, start the PostgreSQL db service!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29efeea0",
   "metadata": {},
   "source": [
    "## 📨 Load Data into PostgreSQL DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc19345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "DB_HOST = \"localhost\"\n",
    "DB_NAME = \"postgres\"\n",
    "DB_USER = \"postgres\"\n",
    "DB_PASSWORD = \"postgres\"\n",
    "DB_PORT = \"5401\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c380af36",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(f'postgresql+psycopg2://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}')\n",
    "\n",
    "try:\n",
    "    # Connect to the db to ensure connection is working\n",
    "    conn_test = psycopg2.connect(\n",
    "        host=DB_HOST,\n",
    "        database=DB_NAME,\n",
    "        user=DB_USER,\n",
    "        password=DB_PASSWORD,\n",
    "        port=DB_PORT\n",
    "    )\n",
    "    conn_test.close()\n",
    "    print(\"Successfully connected to PostgreSQL database!\")\n",
    "\n",
    "    # Load DataFrames\n",
    "    payments_df = pd.read_csv('Payments.csv')\n",
    "    clients_df = pd.read_csv('Clients.csv')\n",
    "\n",
    "    # IMPORTANT: Convert 'transaction_date' from EPOCH to datetime BEFORE loading to DB, in order to have it as a proper timestamp\n",
    "    payments_df['transaction_date'] = pd.to_datetime(payments_df['transaction_date'], unit='s')\n",
    "\n",
    "\n",
    "    # Write DataFrames to PostgreSQL tables\n",
    "    # if_exists: 'replace' will drop the table if it exists and recreate it\n",
    "    # index=False: Do not write the DataFrame index as a column in the database table\n",
    "\n",
    "    print(\"Importing Payments DataFrame to 'payments' table...\")\n",
    "    payments_df.to_sql('payments', engine, if_exists='replace', index=False)\n",
    "    print(\"Payments data imported successfully!\")\n",
    "\n",
    "    print(\"Importing Clients DataFrame to 'clients' table...\")\n",
    "    clients_df.to_sql('clients', engine, if_exists='replace', index=False)\n",
    "    print(\"Clients data imported successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    # Close the engine connection\n",
    "    engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f701836",
   "metadata": {},
   "source": [
    "## 📩 Load Data from PostgreSQL DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c46723",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(f'postgresql+psycopg2://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}')\n",
    "\n",
    "try:\n",
    "    # Read data back from PostgreSQL into pandas DataFrames\n",
    "    payments_df_from_db = pd.read_sql('SELECT * FROM payments', engine)\n",
    "    clients_df_from_db = pd.read_sql('SELECT * FROM clients', engine)\n",
    "\n",
    "    print(\"Data successfully loaded from PostgreSQL into DataFrames for analysis.\")\n",
    "    print(\"\\nPayments DataFrame (from DB) - first 5 rows:\")\n",
    "    print(payments_df_from_db.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "    print(\"\\nClients DataFrame (from DB) - first 5 rows:\")\n",
    "    print(clients_df_from_db.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data from database: {e}\")\n",
    "finally:\n",
    "    engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ccbaf4",
   "metadata": {},
   "source": [
    "## 🧼 Check Data is Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278ae705",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_payments = payments_df_from_db.shape[0]\n",
    "total_clients = clients_df_from_db.shape[0]\n",
    "\n",
    "# Payments DataFrame: Check and list duplicate Transaction IDs\n",
    "print(\"\\nChecking for duplicate Transaction IDs in payments data:\")\n",
    "num_payments_duplicates = payments_df_from_db['transaction_id'].duplicated().sum()\n",
    "print(f\"Number of duplicate Transaction IDs found: {num_payments_duplicates} out of {total_payments}\")\n",
    "\n",
    "if num_payments_duplicates > 0:\n",
    "    # Get all rows where transaction_id is duplicated\n",
    "    duplicate_payments = payments_df_from_db[payments_df_from_db['transaction_id'].duplicated(keep=False)]\n",
    "    print(\"\\nDuplicate Transaction ID rows in Payments DataFrame:\")\n",
    "    # Sort by transaction_id to see duplicates grouped together\n",
    "    print(duplicate_payments.sort_values(by='transaction_id').to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "else:\n",
    "    print(\"No duplicate Transaction IDs found.\")\n",
    "\n",
    "# Clients DataFrame: Check and list duplicate Client IDs\n",
    "print(\"\\nChecking for duplicate Client IDs in clients data:\")\n",
    "num_clients_duplicates = clients_df_from_db['client_id'].duplicated().sum()\n",
    "print(f\"Number of duplicate Client IDs found: {num_clients_duplicates} out of {total_clients}\")\n",
    "\n",
    "if num_clients_duplicates > 0:\n",
    "    # Get all rows where client_id is duplicated\n",
    "    duplicate_clients = clients_df_from_db[clients_df_from_db['client_id'].duplicated(keep=False)]\n",
    "    print(\"\\nDuplicate Client ID rows in Clients DataFrame:\")\n",
    "    # Sort by client_id to see duplicates grouped together\n",
    "    print(duplicate_clients.sort_values(by='client_id').to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "    print(\"\\n--- Payments Associated with Duplicate Client IDs ---\")\n",
    "    # Get the unique client_ids that are duplicated\n",
    "    duplicated_client_ids = duplicate_clients['client_id'].unique()\n",
    "\n",
    "    if len(duplicated_client_ids) > 0:\n",
    "        # Filter the payments_df_from_db to find all payments for these client IDs\n",
    "        payments_for_duplicate_clients = payments_df_from_db[\n",
    "            payments_df_from_db['client_id'].isin(duplicated_client_ids)\n",
    "        ]\n",
    "        print(f\"Found {len(payments_for_duplicate_clients)} out of {total_payments} payments associated with {len(duplicated_client_ids)} duplicate client(s)\")\n",
    "        print(\"\\nAll Payments associated with Duplicate Client IDs:\")\n",
    "        # Sort by client_id and then transaction_date for better readability\n",
    "        print(payments_for_duplicate_clients.sort_values(by=['client_id', 'transaction_date']).to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "    else:\n",
    "        print(\"No payments found for the identified duplicate client IDs (this case should ideally not happen if num_clients_duplicates > 0).\")\n",
    "else:\n",
    "    print(\"No duplicate Client IDs found.\")\n",
    "\n",
    "print(\"\\n--- Duplicate Check Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafe7eeb",
   "metadata": {},
   "source": [
    "Since there are no duplicate transactions, to clean the data:\n",
    "1. deletes all but the oldest client of clients with duplicate client ids\n",
    "2. if any clients have duplicate ids and ages, delete both of them, and all the transactions associated with that client id even if those clients are the oldest of the clients with the duplicate ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b562a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper function to 'keep oldest, delete ties' ---\n",
    "def clean_duplicates_by_age(df, id_col, age_col, name):\n",
    "    \"\"\"\n",
    "    Cleans duplicates in a DataFrame: keeps the single oldest entry for each ID,\n",
    "    discards all entries for an ID if there's a tie for the oldest.\n",
    "    Returns the cleaned DataFrame and a list of IDs whose all entries were discarded.\n",
    "    \"\"\"\n",
    "    # Start with unique IDs\n",
    "    df_cleaned_initial = df[~df[id_col].duplicated(keep=False)].copy()\n",
    "    \n",
    "    # Identify IDs that have duplicates\n",
    "    duplicated_ids = df[df[id_col].duplicated(keep=False)][id_col].unique()\n",
    "    \n",
    "    ids_fully_discarded = []\n",
    "    \n",
    "    if len(duplicated_ids) > 0:\n",
    "        print(f\"\\nProcessing {len(duplicated_ids)} duplicated {name} IDs...\")\n",
    "        for dupe_id in duplicated_ids:\n",
    "            entries = df[df[id_col] == dupe_id].copy()\n",
    "            \n",
    "            # Find the minimum (oldest) value for the age column\n",
    "            oldest_age = entries[age_col].min()\n",
    "\n",
    "            # Filter entries that have this oldest age\n",
    "            oldest_entries = entries[entries[age_col] == oldest_age]\n",
    "            \n",
    "            if len(oldest_entries) == 1:\n",
    "                # If there's a unique oldest entry, keep it\n",
    "                df_cleaned_initial = pd.concat([df_cleaned_initial, oldest_entries], ignore_index=True)\n",
    "            else:\n",
    "                # If there's a tie for the oldest, discard all entries for this ID\n",
    "                ids_fully_discarded.append(dupe_id)\n",
    "                \n",
    "        print(f\"Number of {name} IDs where all tied entries were discarded: {len(ids_fully_discarded)}\")\n",
    "        if ids_fully_discarded:\n",
    "            print(f\"Discarded {name} IDs due to age ties: {ids_fully_discarded}\")\n",
    "    else:\n",
    "        print(f\"No duplicated {name} IDs to process based on age.\")\n",
    "            \n",
    "    return df_cleaned_initial, ids_fully_discarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8415d4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Start Custom Data Cleaning Block ---\n",
    "print(\"\\n--- Starting Custom Data Cleaning ---\")\n",
    "\n",
    "# Make copies to work with, preserving originals from previous steps\n",
    "payments_temp_df = payments_df_from_db.copy()\n",
    "clients_temp_df = clients_df_from_db.copy()\n",
    "\n",
    "# Ensure 'age' columns are in appropriate formats\n",
    "clients_temp_df['entity_year_established'] = pd.to_numeric(clients_temp_df['entity_year_established'], errors='coerce')\n",
    "payments_temp_df['transaction_date'] = pd.to_datetime(payments_temp_df['transaction_date'])\n",
    "\n",
    "# --- Clean Clients DataFrame based on `client_id` and `entity_year_established` ---\n",
    "print(\"\\n--- Cleaning Clients ---\")\n",
    "clients_cleaned_df, discarded_client_ids = clean_duplicates_by_age(\n",
    "    clients_temp_df, 'client_id', 'entity_year_established', 'Client'\n",
    ")\n",
    "clients_initial_rows = clients_temp_df.shape[0]\n",
    "clients_cleaned_rows = clients_cleaned_df.shape[0]\n",
    "clients_removed_count = clients_initial_rows - clients_cleaned_rows\n",
    "print(f\"Total client entries removed: {clients_removed_count}\")\n",
    "print(f\"Clients DataFrame after custom client_id cleaning: {clients_cleaned_df.shape}\")\n",
    "\n",
    "\n",
    "# --- Filter Payments DataFrame based on the cleaned client IDs ---\n",
    "# This step ensures that all transactions belonging to clients that were completely\n",
    "# removed in the previous step (due to tied oldest 'entity_year_established') are also removed.\n",
    "print(\"\\n--- Filtering Payments based on Cleaned Clients ---\")\n",
    "valid_client_ids = clients_cleaned_df['client_id'].unique()\n",
    "payments_filtered_by_clients_df = payments_temp_df[\n",
    "    payments_temp_df['client_id'].isin(valid_client_ids)\n",
    "]\n",
    "payments_removed_by_client_filter_count = payments_temp_df.shape[0] - payments_filtered_by_clients_df.shape[0]\n",
    "print(f\"Removed {payments_removed_by_client_filter_count} payment transactions associated with discarded client IDs.\")\n",
    "print(f\"Payments DataFrame after filtering by valid clients: {payments_filtered_by_clients_df.shape}\")\n",
    "payments_cleaned_df = payments_filtered_by_clients_df.copy()\n",
    "\n",
    "print(\"\\n--- Data Cleaning Complete ---\")\n",
    "print(f\"Original Payments rows: {total_payments} -> Final Cleaned Payments rows: {payments_cleaned_df.shape[0]}\")\n",
    "print(f\"Original Clients rows: {total_clients} -> Final Cleaned Clients rows: {clients_cleaned_df.shape[0]}\")\n",
    "\n",
    "\n",
    "# Display the head of the cleaned DataFrames to confirm\n",
    "print(\"\\nCleaned Payments DataFrame - first 5 rows:\")\n",
    "print(payments_cleaned_df.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "print(\"\\nCleaned Clients DataFrame - first 5 rows:\")\n",
    "print(clients_cleaned_df.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "# You can now proceed with your analysis using 'payments_cleaned_df' and 'clients_cleaned_df'\n",
    "# For example, to merge them for further analysis:\n",
    "# merged_analysis_df = pd.merge(payments_cleaned_df, clients_cleaned_df, on='client_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44d853d",
   "metadata": {},
   "source": [
    "Merge Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de05414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a left merge to keep all payments and bring in client information\n",
    "merged_cleaned_df = pd.merge(payments_cleaned_df, clients_cleaned_df, on='client_id', how='left')\n",
    "\n",
    "# --- Create 'is_default' column ---\n",
    "merged_cleaned_df['is_default'] = merged_cleaned_df['payment_code'].apply(lambda x: 1 if x == 'DEFAULT' else 0)\n",
    "\n",
    "print(\"\\nMerged Cleaned DataFrame with 'is_default' column - first 5 rows:\")\n",
    "print(merged_cleaned_df.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "print(f\"Merged Cleaned DataFrame shape: {merged_cleaned_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92752282",
   "metadata": {},
   "source": [
    "## 🏗️ Feature Enigneering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22fefdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure transaction_date is datetime and entity_year_established is numeric\n",
    "merged_cleaned_df['transaction_date'] = pd.to_datetime(merged_cleaned_df['transaction_date'])\n",
    "merged_cleaned_df['entity_year_established'] = pd.to_numeric(merged_cleaned_df['entity_year_established'])\n",
    "\n",
    "# i) Client Age at Transaction\n",
    "merged_cleaned_df['client_age_at_transaction'] = merged_cleaned_df['transaction_date'].dt.year - merged_cleaned_df['entity_year_established']\n",
    "print(\"\\nAdded 'client_age_at_transaction' to Merged Cleaned DataFrame. Head:\")\n",
    "print(merged_cleaned_df[['transaction_date', 'entity_year_established', 'client_age_at_transaction']].head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "\n",
    "# ii) Temporal Features (month, quarter, day of the week)\n",
    "merged_cleaned_df['transaction_month'] = merged_cleaned_df['transaction_date'].dt.month\n",
    "merged_cleaned_df['transaction_quarter'] = merged_cleaned_df['transaction_date'].dt.quarter\n",
    "merged_cleaned_df['transaction_day_of_week'] = merged_cleaned_df['transaction_date'].dt.dayofweek # Monday=0, Sunday=6\n",
    "print(\"\\nAdded temporal features to Merged Cleaned DataFrame. Head:\")\n",
    "print(merged_cleaned_df[['transaction_date', 'transaction_month', 'transaction_quarter', 'transaction_day_of_week']].head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "\n",
    "# iii) Payment Lag (Time between consecutive payments for a contract)\n",
    "merged_cleaned_df.sort_values(by=['contract_id', 'transaction_date'], inplace=True)\n",
    "merged_cleaned_df['payment_lag_days'] = merged_cleaned_df.groupby('contract_id')['transaction_date'].diff().dt.days\n",
    "print(\"\\nAdded 'payment_lag_days' to Merged Cleaned DataFrame. Head (sorted by contract_id):\")\n",
    "print(merged_cleaned_df[['contract_id', 'transaction_date', 'payment_lag_days']].head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1cfef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iv) Count of Defaults per Client\n",
    "client_default_summary = merged_cleaned_df.groupby('client_id').agg(\n",
    "    total_transactions=('is_default', 'count'),\n",
    "    total_defaults=('is_default', 'sum')\n",
    ").reset_index()\n",
    "client_default_summary['client_default_rate'] = (client_default_summary['total_defaults'] / client_default_summary['total_transactions']) * 100\n",
    "print(\"\\nClient Default Summary - top 5:\")\n",
    "print(client_default_summary.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "# v) Count of Defaults per Contract\n",
    "contract_default_summary = merged_cleaned_df.groupby('contract_id').agg(\n",
    "    total_transactions=('is_default', 'count'),\n",
    "    total_defaults=('is_default', 'sum')\n",
    ").reset_index()\n",
    "contract_default_summary['contract_default_rate'] = (contract_default_summary['total_defaults'] / contract_default_summary['total_transactions']) * 100\n",
    "print(\"\\nContract Default Summary - top 5:\")\n",
    "print(contract_default_summary.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d352673",
   "metadata": {},
   "source": [
    "## 📉 Initial Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d44d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb57e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Payment Amounts\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(merged_cleaned_df['payment_amt'], bins=50, kde=True)\n",
    "plt.title('Distribution of Payment Amounts (Cleaned Data)')\n",
    "plt.xlabel('Payment Amount')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.savefig('payment_amount_distribution_cleaned.png')\n",
    "plt.show()\n",
    "\n",
    "# Without the top 0.75% of payments\n",
    "upper_limit = merged_cleaned_df['payment_amt'].quantile(0.9925)\n",
    "filtered_payments = merged_cleaned_df[merged_cleaned_df['payment_amt'] <= upper_limit]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(filtered_payments['payment_amt'], bins=50, kde=True)\n",
    "plt.title('Distribution of Payment Amounts (Cleaned Data, Excluding top 0.75% of payments)')\n",
    "plt.xlabel('Payment Amount')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.savefig('payment_amount_distribution_cleaned_no_outliers.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4278343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Payment Amounts between those that were defaulted and those that were paid\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='is_default', y='payment_amt', data=merged_cleaned_df)\n",
    "plt.title('Payment Amount Distribution by Default Status (Cleaned Data)')\n",
    "plt.xlabel('Is Default (0: No, 1: Yes)')\n",
    "plt.ylabel('Payment Amount')\n",
    "plt.tight_layout()\n",
    "plt.savefig('payment_amount_by_default_status_cleaned.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfa1d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_type_defaults_cleaned = merged_cleaned_df.groupby('entity_type').agg(\n",
    "    total_transactions=('is_default', 'count'),\n",
    "    defaults=('is_default', 'sum')\n",
    ").reset_index()\n",
    "entity_type_defaults_cleaned['default_rate'] = (entity_type_defaults_cleaned['defaults'] / entity_type_defaults_cleaned['total_transactions']) * 100\n",
    "entity_type_defaults_cleaned = entity_type_defaults_cleaned.sort_values(by='default_rate', ascending=False)\n",
    "\n",
    "print(\"\\nDefault rates by Entity Type (from Cleaned Data):\")\n",
    "print(entity_type_defaults_cleaned.to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='entity_type', y='default_rate', data=entity_type_defaults_cleaned, hue='entity_type', palette='viridis', legend=False)\n",
    "plt.title('Default Rate by Entity Type (Cleaned Data)')\n",
    "plt.xlabel('Entity Type')\n",
    "plt.ylabel('Default Rate (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('default_rate_by_entity_type_cleaned.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6cf61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_year = 20\n",
    "age_bins = list(range(0, max_year)) + [max_year, float('inf')]\n",
    "age_labels = [f\"{i}-{i+1} Years\" for i in range(0, max_year)] + [\"20+ Years\"]\n",
    "\n",
    "merged_cleaned_df['client_age_group'] = pd.cut(\n",
    "    merged_cleaned_df['client_age_at_transaction'],\n",
    "    bins=age_bins,\n",
    "    labels=age_labels,\n",
    "    right=False\n",
    ")\n",
    "\n",
    "age_group_defaults_cleaned = merged_cleaned_df.groupby('client_age_group', observed=True).agg(\n",
    "    total_transactions=('is_default', 'count'),\n",
    "    defaults=('is_default', 'sum')\n",
    ").reset_index()\n",
    "age_group_defaults_cleaned['default_rate'] = (age_group_defaults_cleaned['defaults'] / age_group_defaults_cleaned['total_transactions']) * 100\n",
    "age_group_defaults_cleaned = age_group_defaults_cleaned.sort_values(by='default_rate', ascending=False)\n",
    "\n",
    "print(\"\\nDefault rates by Client Age Group (Cleaned Data):\")\n",
    "print(age_group_defaults_cleaned.to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='client_age_group', y='default_rate', data=age_group_defaults_cleaned, hue='client_age_group', palette='viridis', legend=False, dodge=False)\n",
    "plt.title('Default Rate by Client Age Group at Transaction (Cleaned Data)')\n",
    "plt.xlabel('Client Age Group')\n",
    "plt.ylabel('Default Rate (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('default_rate_by_client_age_group_cleaned.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39c1d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_bins = [0, 4, 7, 10, 13, 16, 19, float('inf')]\n",
    "age_labels = [\"0-3 Years\", \"4-6 Years\", \"7-9 Years\", \"10-12 Years\", \"13-15 Years\", \"16-18 Years\", \"19+ Years\"]\n",
    "\n",
    "merged_cleaned_df['client_age_group'] = pd.cut(\n",
    "    merged_cleaned_df['client_age_at_transaction'],\n",
    "    bins=age_bins,\n",
    "    labels=age_labels,\n",
    "    right=False\n",
    ")\n",
    "\n",
    "age_group_defaults_cleaned = merged_cleaned_df.groupby('client_age_group', observed=True).agg(\n",
    "    total_transactions=('is_default', 'count'),\n",
    "    defaults=('is_default', 'sum')\n",
    ").reset_index()\n",
    "age_group_defaults_cleaned['default_rate'] = (age_group_defaults_cleaned['defaults'] / age_group_defaults_cleaned['total_transactions']) * 100\n",
    "age_group_defaults_cleaned = age_group_defaults_cleaned.sort_values(by='default_rate', ascending=False)\n",
    "\n",
    "print(\"\\nDefault rates by Client Age Group (Cleaned Data):\")\n",
    "print(age_group_defaults_cleaned.to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x='client_age_group', y='default_rate', data=age_group_defaults_cleaned, hue='client_age_group', palette='viridis', legend=False, dodge=False)\n",
    "plt.title('Default Rate by Client Age Group at Transaction (Cleaned Data)')\n",
    "plt.xlabel('Client Age Group')\n",
    "plt.ylabel('Default Rate (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('default_rate_by_client_age_group_cleaned.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd5d360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal Deafults (Month & Quarter)\n",
    "monthly_defaults_cleaned = merged_cleaned_df.groupby('transaction_month').agg(\n",
    "    total_transactions=('is_default', 'count'),\n",
    "    defaults=('is_default', 'sum')\n",
    ").reset_index()\n",
    "monthly_defaults_cleaned['default_rate'] = (monthly_defaults_cleaned['defaults'] / monthly_defaults_cleaned['total_transactions']) * 100\n",
    "\n",
    "print(\"\\nDefault rates by Month (from Cleaned Data):\")\n",
    "print(monthly_defaults_cleaned.sort_values(by='transaction_month').to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(x='transaction_month', y='default_rate', data=monthly_defaults_cleaned, marker='o')\n",
    "plt.title('Default Rate by Transaction Month (Cleaned Data)')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Default Rate (%)')\n",
    "plt.xticks(range(1, 13), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('default_rate_by_month_cleaned.png')\n",
    "plt.show()\n",
    "\n",
    "quarterly_defaults_cleaned = merged_cleaned_df.groupby('transaction_quarter').agg(\n",
    "    total_transactions=('is_default', 'count'),\n",
    "    defaults=('is_default', 'sum')\n",
    ").reset_index()\n",
    "quarterly_defaults_cleaned['default_rate'] = (quarterly_defaults_cleaned['defaults'] / quarterly_defaults_cleaned['total_transactions']) * 100\n",
    "\n",
    "print(\"\\nDefault rates by Quarter (from Cleaned Data):\")\n",
    "print(quarterly_defaults_cleaned.sort_values(by='transaction_quarter').to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x='transaction_quarter', y='default_rate', data=quarterly_defaults_cleaned, hue='transaction_quarter', palette='cividis', legend=False)\n",
    "plt.title('Default Rate by Transaction Quarter (Cleaned Data)')\n",
    "plt.xlabel('Quarter')\n",
    "plt.ylabel('Default Rate (%)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('default_rate_by_quarter_cleaned.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b552140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default rates by Payment Lag Group\n",
    "# Create lag bins\n",
    "lag_bins = [-1, 0, 7, 14, 30, 60, 90, 180, merged_cleaned_df['payment_lag_days'].max() + 1]\n",
    "lag_labels = ['Same Day/Prev Day (0)', '1-7 Days', '8-14 Days', '15-30 Days', '31-60 Days', '61-90 Days', '91-180 Days', '180+ Days']\n",
    "\n",
    "# Exclude NaN values from payment_lag_days as they are first payments of a contract\n",
    "lag_analysis_df_cleaned = merged_cleaned_df.dropna(subset=['payment_lag_days']).copy()\n",
    "lag_analysis_df_cleaned['payment_lag_group'] = pd.cut(\n",
    "    lag_analysis_df_cleaned['payment_lag_days'],\n",
    "    bins=lag_bins,\n",
    "    labels=lag_labels,\n",
    "    right=True\n",
    ")\n",
    "\n",
    "lag_defaults_cleaned = lag_analysis_df_cleaned.groupby('payment_lag_group', observed=False).agg(\n",
    "    total_transactions=('is_default', 'count'),\n",
    "    defaults=('is_default', 'sum')\n",
    ").reset_index()\n",
    "lag_defaults_cleaned['default_rate'] = (lag_defaults_cleaned['defaults'] / lag_defaults_cleaned['total_transactions']) * 100\n",
    "\n",
    "print(\"\\nDefault rates by Payment Lag Group (from Cleaned Data):\")\n",
    "print(lag_defaults_cleaned.sort_values(by='default_rate', ascending=False).to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "ax = sns.barplot(\n",
    "    x='payment_lag_group',\n",
    "    y='default_rate',\n",
    "    data=lag_defaults_cleaned,\n",
    "    hue='payment_lag_group',\n",
    "    palette='plasma'\n",
    ")\n",
    "plt.title('Default Rate by Payment Lag (Days between payments) (Cleaned Data)\\nBar labels show number of defaults in each lag group', fontsize=16)\n",
    "plt.xlabel('Payment Lag Group')\n",
    "plt.ylabel('Default Rate (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Annotate each bar with the raw frequency (defaults)\n",
    "for i, row in lag_defaults_cleaned.iterrows():\n",
    "    ax.text(\n",
    "        i,\n",
    "        row['default_rate'] / 2,\n",
    "        f\"{int(row['defaults'])}\",\n",
    "        ha='center', va='center',\n",
    "        color='white', fontweight='bold', fontsize=12,\n",
    "        bbox=dict(facecolor='black', alpha=0.6, boxstyle='round,pad=0.3')\n",
    "    )\n",
    "\n",
    "plt.savefig('default_rate_by_payment_lag_cleaned.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6653ff53",
   "metadata": {},
   "source": [
    "## ⚒️ Refining Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f49f440",
   "metadata": {},
   "source": [
    "The conditions for a \"rectified\" payment:\n",
    "1. Current transaction's original payment_code is 'DEFAULT'\n",
    "2. Next payment exists (not NaN for next_payment_code_cc)\n",
    "3. Next payment's 'is_default' status is 0 (meaning it was successful/non-default)\n",
    "4. Next payment's amount is the same as the current payment's amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9c9e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c93f177",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_rectify_hour = merged_cleaned_df.copy()\n",
    "merged_df_rectify_hour.sort_values(by=['client_id', 'contract_id', 'transaction_date'], inplace=True)\n",
    "\n",
    "# Create next transaction columns within each client-contract group\n",
    "merged_df_rectify_hour['next_payment_code_cc'] = merged_df_rectify_hour.groupby(['client_id', 'contract_id'])['payment_code'].shift(-1)\n",
    "merged_df_rectify_hour['next_transaction_date_cc'] = merged_df_rectify_hour.groupby(['client_id', 'contract_id'])['transaction_date'].shift(-1)\n",
    "merged_df_rectify_hour['next_payment_amt_cc'] = merged_df_rectify_hour.groupby(['client_id', 'contract_id'])['payment_amt'].shift(-1)\n",
    "merged_df_rectify_hour['next_is_default_cc'] = merged_df_rectify_hour.groupby(['client_id', 'contract_id'])['is_default'].shift(-1)\n",
    "\n",
    "# Calculate time difference to next transaction in hours\n",
    "merged_df_rectify_hour['hours_to_next'] = (\n",
    "    (merged_df_rectify_hour['next_transaction_date_cc'] - merged_df_rectify_hour['transaction_date']).dt.total_seconds() / 3600\n",
    ")\n",
    "\n",
    "# Only consider rows where current payment is DEFAULT and next payment is a successful rectification (same amount, not default)\n",
    "rectified_mask = (\n",
    "    (merged_df_rectify_hour['payment_code'] == 'DEFAULT') &\n",
    "    (merged_df_rectify_hour['next_payment_code_cc'].notna()) &\n",
    "    (merged_df_rectify_hour['next_is_default_cc'] == 0) &\n",
    "    (merged_df_rectify_hour['next_payment_amt_cc'] == merged_df_rectify_hour['payment_amt'])\n",
    ")\n",
    "\n",
    "rectified_df = merged_df_rectify_hour[rectified_mask].copy()\n",
    "rectified_df['hours_to_next_rounded'] = rectified_df['hours_to_next'].round().astype(int)\n",
    "\n",
    "# Count number of rectifying payments for each hour after a default\n",
    "rectify_hour_counts = rectified_df.groupby('hours_to_next_rounded').size().reset_index(name='num_rectifying_payments')\n",
    "\n",
    "print(\"\\nNumber of rectifying payments by hour after a defaulted payment (first 20 rows):\")\n",
    "print(rectify_hour_counts.head(20).to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='hours_to_next_rounded', y='num_rectifying_payments', data=rectify_hour_counts, color='royalblue')\n",
    "plt.title('Number of Rectifying Payments by Hour After Defaulted Payment')\n",
    "plt.xlabel('Hours After Defaulted Payment')\n",
    "plt.ylabel('Number of Rectifying Payments')\n",
    "plt.tight_layout()\n",
    "plt.savefig('rectifying_payments_by_hour.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8cf8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_round2 = merged_cleaned_df.copy()\n",
    "\n",
    "print(\"Created `merged_df_round2` as a copy of `merged_cleaned_df` for further enrichment.\")\n",
    "print(f\"Initial shape of `merged_df_round2`: {merged_df_round2.shape}\")\n",
    "\n",
    "# Ensure the DataFrame is sorted by client_id, contract_id, and transaction_date\n",
    "# This is crucial for accurate 'shift' operations within groups\n",
    "merged_df_round2.sort_values(by=['client_id', 'contract_id', 'transaction_date'], inplace=True)\n",
    "\n",
    "# Create temporary columns for the *next* transaction's details within the same client-contract pair\n",
    "merged_df_round2['next_payment_code_cc'] = merged_df_round2.groupby(['client_id', 'contract_id'])['payment_code'].shift(-1)\n",
    "merged_df_round2['next_transaction_date_cc'] = merged_df_round2.groupby(['client_id', 'contract_id'])['transaction_date'].shift(-1)\n",
    "merged_df_round2['next_payment_amt_cc'] = merged_df_round2.groupby(['client_id', 'contract_id'])['payment_amt'].shift(-1)\n",
    "merged_df_round2['next_is_default_cc'] = merged_df_round2.groupby(['client_id', 'contract_id'])['is_default'].shift(-1) # Use the already refined is_default\n",
    "\n",
    "\n",
    "# Calculate the time difference to the next transaction for the same client-contract pair\n",
    "merged_df_round2['time_to_next_transaction_cc'] = merged_df_round2['next_transaction_date_cc'] - merged_df_round2['transaction_date']\n",
    "\n",
    "# Convert timedelta to total hours\n",
    "merged_df_round2['hours_to_next_cc'] = merged_df_round2['time_to_next_transaction_cc'].dt.total_seconds() / 3600\n",
    "\n",
    "\n",
    "# Define the conditions for a \"rectified\" payment\n",
    "# 1. Current transaction's original payment_code is 'DEFAULT'\n",
    "# 2. Next payment exists (not NaN for next_payment_code_cc)\n",
    "# 3. Next payment's 'is_default' status is 0 (meaning it was successful/non-default after initial refinement)\n",
    "# 4. Next payment's amount is the same as the current payment's amount\n",
    "rectified_conditions = (\n",
    "    (merged_df_round2['payment_code'] == 'DEFAULT') & # Check original payment_code for DEFAULT\n",
    "    (merged_df_round2['next_payment_code_cc'].notna()) & # Ensure there is a next payment\n",
    "    (merged_df_round2['next_is_default_cc'] == 0) & # The next payment was not a default (after initial refinement)\n",
    "    (merged_df_round2['next_payment_amt_cc'] == merged_df_round2['payment_amt']) # Amounts match\n",
    ")\n",
    "\n",
    "# Initialize the new column 'hours_until_rectified'\n",
    "# For transactions that are NOT defaults or don't meet rectification criteria, set to np.inf\n",
    "merged_df_round2['hours_until_rectified'] = np.inf\n",
    "\n",
    "# For transactions that ARE defaults and meet the rectification criteria, set to the calculated hours\n",
    "merged_df_round2.loc[rectified_conditions, 'hours_until_rectified'] = \\\n",
    "    merged_df_round2.loc[rectified_conditions, 'hours_to_next_cc']\n",
    "\n",
    "num_rectified_defaults_round2 = merged_df_round2[merged_df_round2['hours_until_rectified'] != np.inf].shape[0]\n",
    "print(f\"\\nCreated 'hours_until_rectified' flag.\")\n",
    "print(f\"Found {num_rectified_defaults_round2} instances of defaults that were rectified with matching amounts.\")\n",
    "print(\"For non-rectified defaults or non-default transactions, 'hours_until_rectified' is set to infinity (np.inf).\")\n",
    "\n",
    "\n",
    "# Clean up the temporary columns\n",
    "merged_df_round2.drop(columns=[\n",
    "    'next_payment_code_cc',\n",
    "    'next_transaction_date_cc',\n",
    "    'next_payment_amt_cc',\n",
    "    'next_is_default_cc',\n",
    "    'time_to_next_transaction_cc',\n",
    "    'hours_to_next_cc'\n",
    "], inplace=True, errors='ignore')\n",
    "\n",
    "print(\"\\n`merged_df_round2` after adding 'hours_until_rectified' flag - first 10 rows:\")\n",
    "# Display relevant columns to confirm the new flag\n",
    "display_cols_flag_round2 = [\n",
    "    'client_id', 'contract_id', 'transaction_date', 'payment_code', 'payment_amt', 'is_default',\n",
    "    'hours_until_rectified'\n",
    "]\n",
    "print(merged_df_round2[display_cols_flag_round2].head(10).to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "rectified_payments_only = merged_df_round2[merged_df_round2['hours_until_rectified'] != np.inf]\n",
    "\n",
    "if not rectified_payments_only.empty:\n",
    "    max_rectification_hours = rectified_payments_only['hours_until_rectified'].max()\n",
    "\n",
    "    # Convert total hours into days and remaining hours\n",
    "    max_days = int(max_rectification_hours // 24)\n",
    "    max_remaining_hours = max_rectification_hours % 24\n",
    "\n",
    "    print(f\"\\nMaximum rectification time observed: {max_days} days and {max_remaining_hours:.2f} hours.\")\n",
    "else:\n",
    "    print(\"\\nNo rectified payments found to calculate maximum rectification time.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3de046",
   "metadata": {},
   "source": [
    "## 📈 Round 2 of Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051b37db",
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_hours_threshold = 168\n",
    "\n",
    "# Filter for rectified payments that are within the specified constant_hours_threshold\n",
    "rectified_within_threshold = merged_df_round2[\n",
    "    (merged_df_round2['hours_until_rectified'] != np.inf) &\n",
    "    (merged_df_round2['hours_until_rectified'] < constant_hours_threshold)\n",
    "]\n",
    "\n",
    "num_rectified_payments_within_threshold = rectified_within_threshold.shape[0]\n",
    "\n",
    "print(f\"\\nNumber of rectifying payments that happened within {constant_hours_threshold} hours: {num_rectified_payments_within_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d5b76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new 'is_default_round2' column in merged_df_round2\n",
    "# Initialize it with the original 'is_default' values\n",
    "merged_df_round2['is_default_round2'] = merged_df_round2['is_default']\n",
    "\n",
    "# Identify defaults that are rectified within the constant_hours_threshold\n",
    "# These are the ones where original 'is_default' was 1, and 'hours_until_rectified' is within the threshold.\n",
    "rectified_within_threshold_mask = (\n",
    "    (merged_df_round2['is_default'] == 1) &\n",
    "    (merged_df_round2['hours_until_rectified'] != np.inf) &\n",
    "    (merged_df_round2['hours_until_rectified'] <= constant_hours_threshold)\n",
    ")\n",
    "\n",
    "# For these specific cases, set 'is_default_round2' to 0\n",
    "merged_df_round2.loc[rectified_within_threshold_mask, 'is_default_round2'] = 0\n",
    "\n",
    "print(f\"\\nCreated 'is_default_round2' column in `merged_df_round2`.\")\n",
    "print(f\"Defaults rectified within {constant_hours_threshold} hours are now considered non-defaults (0) in this new flag.\")\n",
    "\n",
    "# Display a few rows to show the effect of the new flag, especially around defaults\n",
    "display_cols_round2 = [\n",
    "    'client_id', 'contract_id', 'transaction_date', 'payment_code', 'payment_amt',\n",
    "    'is_default', 'hours_until_rectified', 'is_default_round2'\n",
    "]\n",
    "print(\"\\n`merged_df_round2` with 'is_default_round2' - first 10 rows (focus on default related):\")\n",
    "print(merged_df_round2[display_cols_round2].head(10).to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "# Optional: Check how many defaults changed their status\n",
    "num_original_defaults = merged_df_round2['is_default'].sum()\n",
    "num_defaults_round2 = merged_df_round2['is_default_round2'].sum()\n",
    "num_defaults_reclassified = num_original_defaults - num_defaults_round2\n",
    "print(f\"\\nOriginal number of defaults: {num_original_defaults}\")\n",
    "print(f\"Number of defaults in Round 2 Analysis (after reclassification): {num_defaults_round2}\")\n",
    "print(f\"Number of defaults reclassified as non-defaults (rectified within {constant_hours_threshold} hours): {num_defaults_reclassified}\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Re-analyzing Distribution of Payment Amounts\n",
    "\n",
    "# %%\n",
    "# Distribution of Payment Amounts (Histograms)\n",
    "# These plots visualize 'payment_amt' directly and are not affected by 'is_default_round2'\n",
    "# However, we re-generate them for completeness of the Round 2 Analysis section.\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(merged_df_round2['payment_amt'], bins=50, kde=True)\n",
    "plt.title('Distribution of Payment Amounts (Round 2 Analysis)')\n",
    "plt.xlabel('Payment Amount')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.savefig('payment_amount_distribution_round2.png')\n",
    "plt.show()\n",
    "\n",
    "# Without the top 0.75% of payments\n",
    "upper_limit_round2 = merged_df_round2['payment_amt'].quantile(0.9925)\n",
    "filtered_payments_round2 = merged_df_round2[merged_df_round2['payment_amt'] <= upper_limit_round2]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(filtered_payments_round2['payment_amt'], bins=50, kde=True)\n",
    "plt.title('Distribution of Payment Amounts (Round 2 Analysis, Excluding top 0.75% of payments)')\n",
    "plt.xlabel('Payment Amount')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.savefig('payment_amount_distribution_round2_no_outliers.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# %%\n",
    "# Distribution of Payment Amounts between those that were defaulted (Round 2) and those that were paid\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='is_default_round2', y='payment_amt', data=merged_df_round2)\n",
    "plt.title('Payment Amount Distribution by Default Status (Round 2 Analysis - Excl. Rectified within 168hrs)')\n",
    "plt.xlabel('Is Default (0: No, 1: Yes)')\n",
    "plt.ylabel('Payment Amount')\n",
    "plt.tight_layout()\n",
    "plt.savefig('payment_amount_by_default_status_round2.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Re-analyzing Default Rates by Entity Type\n",
    "\n",
    "# %%\n",
    "entity_type_defaults_round2 = merged_df_round2.groupby('entity_type').agg(\n",
    "    total_transactions=('is_default_round2', 'count'),\n",
    "    defaults=('is_default_round2', 'sum')\n",
    ").reset_index()\n",
    "entity_type_defaults_round2['default_rate'] = (entity_type_defaults_round2['defaults'] / entity_type_defaults_round2['total_transactions']) * 100\n",
    "entity_type_defaults_round2 = entity_type_defaults_round2.sort_values(by='default_rate', ascending=False)\n",
    "\n",
    "print(\"\\nDefault rates by Entity Type (Round 2 Analysis - Excl. Rectified within 168hrs):\")\n",
    "print(entity_type_defaults_round2.to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='entity_type', y='default_rate', data=entity_type_defaults_round2, hue='entity_type', palette='viridis', legend=False)\n",
    "plt.title(f'Default Rate by Entity Type (Round 2 Analysis - Excl. Rectified within {constant_hours_threshold}hrs)')\n",
    "plt.xlabel('Entity Type')\n",
    "plt.ylabel('Default Rate (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('default_rate_by_entity_type_round2.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Re-analyzing Default Rates by Client Age Group\n",
    "\n",
    "# %%\n",
    "# Re-create client_age_group in merged_df_round2 to ensure consistency with initial analysis\n",
    "# (even though it should already be there, good practice for self-contained blocks)\n",
    "age_bins_round2 = [0, 4, 7, 10, 13, 16, 19, float('inf')]\n",
    "age_labels_round2 = [\"0-3 Years\", \"4-6 Years\", \"7-9 Years\", \"10-12 Years\", \"13-15 Years\", \"16-18 Years\", \"19+ Years\"]\n",
    "\n",
    "merged_df_round2['client_age_group'] = pd.cut(\n",
    "    merged_df_round2['client_age_at_transaction'],\n",
    "    bins=age_bins_round2,\n",
    "    labels=age_labels_round2,\n",
    "    right=False\n",
    ")\n",
    "\n",
    "age_group_defaults_round2 = merged_df_round2.groupby('client_age_group', observed=True).agg(\n",
    "    total_transactions=('is_default_round2', 'count'),\n",
    "    defaults=('is_default_round2', 'sum')\n",
    ").reset_index()\n",
    "age_group_defaults_round2['default_rate'] = (age_group_defaults_round2['defaults'] / age_group_defaults_round2['total_transactions']) * 100\n",
    "age_group_defaults_round2 = age_group_defaults_round2.sort_values(by='default_rate', ascending=False)\n",
    "\n",
    "print(f\"\\nDefault rates by Client Age Group (Round 2 Analysis - Excl. Rectified within {constant_hours_threshold}hrs):\")\n",
    "print(age_group_defaults_round2.to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x='client_age_group', y='default_rate', data=age_group_defaults_round2, hue='client_age_group', palette='viridis', legend=False, dodge=False)\n",
    "plt.title(f'Default Rate by Client Age Group at Transaction (Round 2 Analysis - Excl. Rectified within {constant_hours_threshold}hrs)')\n",
    "plt.xlabel('Client Age Group')\n",
    "plt.ylabel('Default Rate (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('default_rate_by_client_age_group_round2.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Re-analyzing Temporal Defaults (Month & Quarter)\n",
    "\n",
    "# %%\n",
    "# Monthly Defaults\n",
    "monthly_defaults_round2 = merged_df_round2.groupby('transaction_month').agg(\n",
    "    total_transactions=('is_default_round2', 'count'),\n",
    "    defaults=('is_default_round2', 'sum')\n",
    ").reset_index()\n",
    "monthly_defaults_round2['default_rate'] = (monthly_defaults_round2['defaults'] / monthly_defaults_round2['total_transactions']) * 100\n",
    "\n",
    "print(f\"\\nDefault rates by Month (Round 2 Analysis - Excl. Rectified within {constant_hours_threshold}hrs):\")\n",
    "print(monthly_defaults_round2.sort_values(by='transaction_month').to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(x='transaction_month', y='default_rate', data=monthly_defaults_round2, marker='o')\n",
    "plt.title(f'Default Rate by Transaction Month (Round 2 Analysis - Excl. Rectified within {constant_hours_threshold}hrs)')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Default Rate (%)')\n",
    "plt.xticks(range(1, 13), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('default_rate_by_month_round2.png')\n",
    "plt.show()\n",
    "\n",
    "# Quarterly Defaults\n",
    "quarterly_defaults_round2 = merged_df_round2.groupby('transaction_quarter').agg(\n",
    "    total_transactions=('is_default_round2', 'count'),\n",
    "    defaults=('is_default_round2', 'sum')\n",
    ").reset_index()\n",
    "quarterly_defaults_round2['default_rate'] = (quarterly_defaults_round2['defaults'] / quarterly_defaults_round2['total_transactions']) * 100\n",
    "\n",
    "print(f\"\\nDefault rates by Quarter (Round 2 Analysis - Excl. Rectified within {constant_hours_threshold}hrs):\")\n",
    "print(quarterly_defaults_round2.sort_values(by='transaction_quarter').to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x='transaction_quarter', y='default_rate', data=quarterly_defaults_round2, hue='transaction_quarter', palette='cividis', legend=False)\n",
    "plt.title(f'Default Rate by Transaction Quarter (Round 2 Analysis - Excl. Rectified within {constant_hours_threshold}hrs)')\n",
    "plt.xlabel('Quarter')\n",
    "plt.ylabel('Default Rate (%)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('default_rate_by_quarter_round2.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Re-analyzing Default Rates by Payment Lag Group\n",
    "\n",
    "# %%\n",
    "# Create lag bins for merged_df_round2\n",
    "lag_bins_round2 = [-1, 0, 7, 14, 30, 60, 90, 180, merged_df_round2['payment_lag_days'].max() + 1]\n",
    "lag_labels_round2 = ['Same Day/Prev Day (0)', '1-7 Days', '8-14 Days', '15-30 Days', '31-60 Days', '61-90 Days', '91-180 Days', '180+ Days']\n",
    "\n",
    "# Exclude NaN values from payment_lag_days as they are first payments of a contract\n",
    "lag_analysis_df_round2 = merged_df_round2.dropna(subset=['payment_lag_days']).copy()\n",
    "lag_analysis_df_round2['payment_lag_group'] = pd.cut(\n",
    "    lag_analysis_df_round2['payment_lag_days'],\n",
    "    bins=lag_bins_round2,\n",
    "    labels=lag_labels_round2,\n",
    "    right=True\n",
    ")\n",
    "\n",
    "lag_defaults_round2 = lag_analysis_df_round2.groupby('payment_lag_group', observed=False).agg(\n",
    "    total_transactions=('is_default_round2', 'count'),\n",
    "    defaults=('is_default_round2', 'sum')\n",
    ").reset_index()\n",
    "lag_defaults_round2['default_rate'] = (lag_defaults_round2['defaults'] / lag_defaults_round2['total_transactions']) * 100\n",
    "\n",
    "print(f\"\\nDefault rates by Payment Lag Group (Round 2 Analysis - Excl. Rectified within {constant_hours_threshold}hrs):\")\n",
    "print(lag_defaults_round2.sort_values(by='default_rate', ascending=False).to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "ax_round2 = sns.barplot(\n",
    "    x='payment_lag_group',\n",
    "    y='default_rate',\n",
    "    data=lag_defaults_round2,\n",
    "    hue='payment_lag_group',\n",
    "    palette='plasma',\n",
    "    legend=False\n",
    ")\n",
    "plt.title(f'Default Rate by Payment Lag (Days between payments) (Round 2 Analysis - Excl. Rectified within {constant_hours_threshold}hrs)\\nBar labels show number of defaults in each lag group', fontsize=16)\n",
    "plt.xlabel('Payment Lag Group')\n",
    "plt.ylabel('Default Rate (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Annotate each bar with the raw frequency (defaults)\n",
    "for i, row in lag_defaults_round2.iterrows():\n",
    "    ax_round2.text(\n",
    "        i,\n",
    "        row['default_rate'] / 2,\n",
    "        f\"{int(row['defaults'])}\",\n",
    "        ha='center', va='center',\n",
    "        color='white', fontweight='bold', fontsize=12,\n",
    "        bbox=dict(facecolor='black', alpha=0.6, boxstyle='round,pad=0.3')\n",
    "    )\n",
    "\n",
    "plt.savefig('default_rate_by_payment_lag_round2.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
